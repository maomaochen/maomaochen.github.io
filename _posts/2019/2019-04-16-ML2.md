---
layout:     post
title:      周志华《机器学习》第二章笔记
subtitle:   ML Chapter2
date:       2019-4-16
author:     maomaochen
header-img: img/post-bg-rwd.jpg
keywords_post:  "ML,周志华,笔记"
catalog: true
tags:
    - ML
    - 读书笔记
    - 《机器学习》周志华
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head> 

# 第二章 模型评估与选择

## 经验误差与过拟合

分类错误的样本数占样本总数的比例称为**“错误率”**（error rate）,在$m$个样本中有$a$个样本分类错误，则错误率为$E=a/m$。与此相对应将$1-a/m$称为**“精度”**（accuracy）。

学习器的实际预测输出与样本的真实输出之间的差异称为**“误差”**（error，均指误差期望）。在训练集上的误差称为**“训练误差”**（training error）或**“经验误差”**（empirical error），在新样本上的误差称为**“泛化误差”**（generalization error）。

***目标***：希望得到泛化误差更小的学习器，实际能做的是努力使经验误差最小化。

**“过拟合”**（overfitting）:学习器把样本学习的太好，将训练样本自身的一些贴点当作所有潜在样本都会具有的一般性质。`学太好`

**“欠拟合”**（underfiiting）:对训练样本的一般性质尚未学号。`未学好`

欠拟合较为容易克服（增加训练轮数等），过拟合是机器学习面临的关键障碍，**过拟合是无法彻底避免的**。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/01.png)

**“模型选择”**（model selection）问题：选用哪一个学习算法、使用哪一种参数配置？

## 评估方法

需要一个**“测试集”**（testing set）测试学习器对新样本的判别能力，以测试集上的**“测试误差”**（testing error）作为泛化误差的近似。

> 假设测试样本是从样本真实分布中独立同分布采样而得。
>
> **测试集尽量与训练集互斥**。

如果只有一个包含$m$个样例的数据集：

$$D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \dots,\left(\boldsymbol{x}_{m}, y_{m}\right)\}\right.$$

通过对$D$的处理，从中产生训练集$S$和测试集$T$。下面介绍常见做法。

### 留出法

**“留出法”**（hold-out）直接将数据集$D$划分为两个互斥集合，其中一个为训练集$S$，另一个为测试集$T$，即：

$$D=S \cup T, S \cap T=\varnothing​$$

> **以二分类为例**
>
> $D$包含1000个样本，划分$S$包含700个样本，$T$包含300个样本。用$S$训练后，如果模型在$T$上有90个样本分类错误，那么其错误率为（90/300）×100% = 30%，相应的精度为1-30%=70%。

***注意***：训练（测试）集的划分要保持数据分布的一致性，避免数据划分过程引入额外的偏差。从采样（sampling）角度来看，保留类别比例的采样方式为“分层采样”（stratified sampling）。

**单次使用留出法得到的估计结果往往不够可靠**（在给定数据集样本比例的情况下，仍有对$D$的多种划分方式，这都会对评估结果产生影响）。在使用留出法时，要采用若干次随机划分、重复进行试验评估后去平均值作为留出法的评估结果。

> 一般将数据集2/3~4/5的样本用于训练。

### 交叉验证法

**交叉验证法**（cross validation）将数据集$D$划分为$k$个大小相似的互斥子集（保证每个子集数据分布的一致性）：

$$D=D_{1} \cup D_{2} \cup \ldots \cup D_{k}, D_{i} \cap D_{j}=\varnothing(i \neq j)​$$

每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集。这样进行$k$次训练和测试，最终返回$k$个测试结果的均值。

交叉验证法又被称为**$k$折交叉验证**（k-fold cross validation），最常用的$k$为10，此时称为10折交叉验证。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/02.png)

> 交叉验证法评估结果的稳定性和保真性很大程度上取决于$k$的取值。

$k$折交叉验证通常要随机使用不同的划分重复$p$次（减小样本划分不同引入的差别）。结果是$p$次$k$折交叉验证结果的均值。

**“留一法”**（Leave-One-Out，LOO）:数据集$D$包含$m$个样本，令$k=m$，特殊的交叉验证法，不受随机样本划分的限制。

> 留一法的评估结果往往较为准确。但是在数据集较大时，计算开销难以忍受。根据“没有免费的午餐定理”（NFL），其结果不一定比其他评估方法准确。

### 自助法

**“自助法”**（bootstrapping）给定有$m$个样本的数据集$D$，对其进行$m$次有放回的采样（每次随机挑一个放入$D’$，但样本还在$D$中），得到包含$m$个样本的数据集$D’$，样本在$m$次采样中始终不被采样的概率为：

$$\lim _{m \rightarrow \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368$$

也就是说初始数据集$D$中有36.8%的样本不出现在$D’$中，可用$D’$作训练集，$D \backslash D'$作为测试集。实际评估模型与期望评估模型都使用$m$个训练样本，而且还有1/3没在训练集中出现的样本用于测试。这样的测试结果称为“包外估计”（out-of-bagestimate）。

自助法在数据集较小、难以有效划分训练集、测试集时很有用。因为能产生多个不同的训练集，对集成学习有很大好处。

问题在于它改变了初始数据集的分布，所以在数据量足够时一般不使用。

### 调参与最终模型

调参与算法选择没什么本质的区别，但因为学习算法很多参数是在实数范围内取值，对每种参数配置都训练出模型是不可能的。

在训练时因为只使用了一部分数据，所以在模型选择完后，算法与参数已定，需要用完整的数据集重新训练模型。

真实场景中，用**测试集**评估泛化能力，把训练数据划分为**训练集**和**验证集**，基于验证集上的性能进行模型选择和调参。

## 性能度量




























> 参考：周志华《机器学习》第二章



<br>