---
layout:     post
title:      周志华《机器学习》第二章笔记
subtitle:   ML Chapter2
date:       2019-4-16
author:     maomaochen
header-img: img/post-bg-rwd.jpg
keywords_post:  "ML,周志华,笔记"
catalog: true
tags:
    - ML
    - 读书笔记
    - 《机器学习》周志华
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head> 

# 第二章 模型评估与选择

## 经验误差与过拟合

分类错误的样本数占样本总数的比例称为**“错误率”**（error rate）,在$m$个样本中有$a$个样本分类错误，则错误率为$E=a/m$。与此相对应将$1-a/m$称为**“精度”**（accuracy）。

学习器的实际预测输出与样本的真实输出之间的差异称为**“误差”**（error，均指误差期望）。在训练集上的误差称为**“训练误差”**（training error）或**“经验误差”**（empirical error），在新样本上的误差称为**“泛化误差”**（generalization error）。

***目标***：希望得到泛化误差更小的学习器，实际能做的是努力使经验误差最小化。

**“过拟合”**（overfitting）:学习器把样本学习的太好，将训练样本自身的一些贴点当作所有潜在样本都会具有的一般性质。`学太好`

**“欠拟合”**（underfiiting）:对训练样本的一般性质尚未学号。`未学好`

欠拟合较为容易克服（增加训练轮数等），过拟合是机器学习面临的关键障碍，**过拟合是无法彻底避免的**。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/01.png)

**“模型选择”**（model selection）问题：选用哪一个学习算法、使用哪一种参数配置？

## 评估方法

需要一个**“测试集”**（testing set）测试学习器对新样本的判别能力，以测试集上的**“测试误差”**（testing error）作为泛化误差的近似。

> 假设测试样本是从样本真实分布中独立同分布采样而得。
>
> **测试集尽量与训练集互斥**。

如果只有一个包含$m$个样例的数据集：

$$D=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \dots,\left(\boldsymbol{x}_{m}, y_{m}\right)\}\right.$$

通过对$D$的处理，从中产生训练集$S$和测试集$T$。下面介绍常见做法。

### 留出法

**“留出法”**（hold-out）直接将数据集$D$划分为两个互斥集合，其中一个为训练集$S$，另一个为测试集$T$，即：

$$D=S \cup T, S \cap T=\varnothing$$

> **以二分类为例**
>
> $D$包含1000个样本，划分$S$包含700个样本，$T$包含300个样本。用$S$训练后，如果模型在$T$上有90个样本分类错误，那么其错误率为（90/300）×100% = 30%，相应的精度为1-30%=70%。

***注意***：训练（测试）集的划分要保持数据分布的一致性，避免数据划分过程引入额外的偏差。从采样（sampling）角度来看，保留类别比例的采样方式为“分层采样”（stratified sampling）。

**单次使用留出法得到的估计结果往往不够可靠**（在给定数据集样本比例的情况下，仍有对$D$的多种划分方式，这都会对评估结果产生影响）。在使用留出法时，要采用若干次随机划分、重复进行试验评估后去平均值作为留出法的评估结果。

> 一般将数据集2/3~4/5的样本用于训练。

### 交叉验证法

**交叉验证法**（cross validation）将数据集$D$划分为$k$个大小相似的互斥子集（保证每个子集数据分布的一致性）：

$$D=D_{1} \cup D_{2} \cup \ldots \cup D_{k}, D_{i} \cap D_{j}=\varnothing(i \neq j)$$

每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集。这样进行$k$次训练和测试，最终返回$k$个测试结果的均值。

交叉验证法又被称为**$k$折交叉验证**（k-fold cross validation），最常用的$k$为10，此时称为10折交叉验证。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/02.png)

> 交叉验证法评估结果的稳定性和保真性很大程度上取决于$k$的取值。

$k$折交叉验证通常要随机使用不同的划分重复$p$次（减小样本划分不同引入的差别）。结果是$p$次$k$折交叉验证结果的均值。

**“留一法”**（Leave-One-Out，LOO）:数据集$D$包含$m$个样本，令$k=m$，特殊的交叉验证法，不受随机样本划分的限制。

> 留一法的评估结果往往较为准确。但是在数据集较大时，计算开销难以忍受。根据“没有免费的午餐定理”（NFL），其结果不一定比其他评估方法准确。

### 自助法

**“自助法”**（bootstrapping）给定有$m$个样本的数据集$D$，对其进行$m$次有放回的采样（每次随机挑一个放入$D’$，但样本还在$D$中），得到包含$m$个样本的数据集$D’$，样本在$m$次采样中始终不被采样的概率为：

$$\lim _{m \rightarrow \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368$$

也就是说初始数据集$D$中有36.8%的样本不出现在$D’$中，可用$D’$作训练集，$D \backslash D'$作为测试集。实际评估模型与期望评估模型都使用$m$个训练样本，而且还有1/3没在训练集中出现的样本用于测试。这样的测试结果称为“包外估计”（out-of-bagestimate）。

自助法在数据集较小、难以有效划分训练集、测试集时很有用。因为能产生多个不同的训练集，对集成学习有很大好处。

问题在于它改变了初始数据集的分布，所以在数据量足够时一般不使用。

### 调参与最终模型

调参与算法选择没什么本质的区别，但因为学习算法很多参数是在实数范围内取值，对每种参数配置都训练出模型是不可能的。

在训练时因为只使用了一部分数据，所以在模型选择完后，算法与参数已定，需要用完整的数据集重新训练模型。

真实场景中，用**测试集**评估泛化能力，把训练数据划分为**训练集**和**验证集**，基于验证集上的性能进行模型选择和调参。

## 性能度量

有了有效可行的实验估计方法，还需要有衡量模型泛化能力的评估标准——**性能度量**（performance measure）。

下面主要介绍*分类任务*中常用的性能度量。

### 错误率与精度

**错误率**是分类错误的样本数占样本总数的比例。**精度**是分类正确的样本数占样本总数的比例。对数据集$D$，错误率定义为：

$$E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)$$

精度定义为：

$$\begin{aligned} \operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\ &=1-E(f ; D) \end{aligned}$$

一般的，对于数据分布$D$和概率密度函数$p(\cdot)$，错误率与精度分别为：

$$E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$$

$$\begin{aligned} \operatorname{acc}(f ; \mathcal{D}) &=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\ &=1-E(f ; \mathcal{D}) \end{aligned}$$

### 查准率、查全率与$F_1$

二分类任务中样本可依据真实类别和预测类别组合划分为4类：

* 真正例$TP$（true positive）：预测为正，真实为正
* 假正例$FP$（false positive）：预测为正，真实为反
* 真反例$TN$（true negative）：预测为反，真实为反
* 假反例$FN$（false negative）：预测为反，真实为正

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/03.png)

查准率$P$——预测为正的样本中有多少真的为正例：

$$P=\frac{T P}{T P+F P}$$

查全率$R$——所有为正例的样本中有多少被预测为正例：

$$R=\frac{T P}{T P+F N}$$

> 在信息检索中，查准率关心的是在所有被检索出来的信息中用户感兴趣的比例，而查全率关心的是用户感兴趣的信息有多少被检索出来了。**查准率与查全率是互为矛盾的度量**。

**“$P-R$曲线”**综合考虑了查准率和查全率。按预测结果对样本进行排序，排前面的最可能是正例，后面的最不可能是正例，按顺序逐个将样本作为正例进行预测，计算出对应的查全率、查准率。以查准率为纵轴，查全率为横轴，即可绘制$P-R$曲线，对应的图为$P-R$图。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/04.png)

一个学习器的$P-R$曲线被另一个完全包住，则后者性能优于前者（上图A优于C）。对于交叉的两条曲线，可以用曲线下面积比较（较难）。一个用来综合评价$P、R$值的度量是**“平衡点”**（Break-Even Point，BEP，查准率=查全率时的取值），上图基于BEP，可判定A优于B。

更复杂的综合考虑$P、R$值的时$F_1$度量：

$$F_1 = \frac{2\times P\times R}{P+R} = \frac{2\times TP}{样例总数+TP-TN}$$

$F_1$的一般度量是$F_β$，表达对查准率/查全率不同的偏好：

$$F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}$$

当$β>0$度量了查全率对查准率的相对重要性：

* $β>1$时查全率影响更大
* $β<1$时查准率影响更大

`如何在n个二分类混淆矩阵上综合考虑查准率和查全率？`

1.先在个混淆矩阵上分别计算查准率和查全率，记为：

$$\left(P_{1}, R_{1}\right),\left(P_{2}, R_{2}\right), \ldots,\left(P_{n}, R_{n}\right)$$

计算平均值得到“宏查准率”（$macro-P$），“宏查全率”（$macro-R$），“宏$F_1$”（$macro-F_1$）:

$$\begin{array}{c}{\operatorname{macro-P}=\frac{1}{n} \sum_{i=1}^{n} P_{i}} \\ {\operatorname{macro-R}=\frac{1}{n} \sum_{i=1}^{n} R_{i}} \\ {\operatorname{macro-F} 1=\frac{2 \times \operatorname{macro}-P \times \operatorname{macro}-R}{\operatorname{macro}-P+\operatorname{macro}-R}}\end{array}$$

2.先将混淆矩阵对应元素平均，得到$TP、FP、TN、FN$的均值：

$$\overline{T P}, \overline{F P}, \overline{T N}, \overline{F N}$$

基于上述均值计算“微查准率”（$micro-P$），“微查全率”（$micro-R$），“微$F_1$”（$micro-F_1$）。公式略。

### $ROC$与$AUC$

根据预测结果对测试样本进行排序，分类过程就是设置**截断点**将样本分为两部分。根据任务的需求采用不同的截断点，排序本身质量的好坏，体现了“一般情况下”泛化性能的好坏（即在不同任务下的性能）。

**ROC**（Receiver Operating Characterstic）曲线基于上述角度研究学习器的泛化性能。与$P-R$曲线类似，逐个将样本作为正例进行预测，以“真正例率”（True Positive Rate，TPR）为纵轴，“假正例率”（False Positve Rate，FPR）为横轴。

* TPR：正例样本中被判定为正例的样本比例
* FPR：假例样本中被判定为正例的样本比例

$$\begin{aligned} \mathrm{TPR} &=\frac{T P}{T P+F N} \\ \mathrm{FPR} &=\frac{F P}{T N+F P} \end{aligned}$$

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/05.png)

ROC曲线对应图称为ROC图，其对角线对应于“随机猜测”模型，点$(0,1)$对应于将所有正例排在反例之前的“理想模型”。

一个学习器的ROC曲线被另一个学习器的曲线完全包住，则后者性能优于前者。若交叉，可根据曲线下面积**AUC**（Area Under ROC Curve）判断。假设ROC曲线由如下点确定（$x_1=0,x_m=1$）：

$$\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{m}, y_{m}\right)\right\}$$

则AUC估算为：

$$\mathrm{AUC}=\frac{1}{2} \sum_{i=1}^{m-1}\left(x_{i+1}-x_{i}\right) \cdot\left(y_{i}+y_{i+1}\right)$$

> AUC考虑样本预测的排序质量，其与排序误差有紧密联系。给定$m^+$个正例和$m^-$个反例，$D^+、D^-$分别代表正、反集合，排序“损失”定义为：
>
> $$\ell_{\text {rank}}=\frac{1}{m^{+} m^{-}} \sum_{x^{+} \in D^{+}} \sum_{x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)$$
>
> 对于一对正反例，正例预测值小于假例，记一个“罚分”，相等则记0.5个“罚分”，上述结果对应的时ROC曲线之上的面积。

### 代价敏感错误率与代价曲线

为权衡不同类型错误造成的不同损失，可为错误赋予**“非均等代价”**（unequal cost）。以二分类为例，可构建**代价矩阵**（cost matrix），若将0类判别为1类造成造成更大的损失，则$cost_{01}>cost_{10}$。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/06.png)

在非均等代价下，期望得到**最小化“总体代价”**（total cost）。如果上表第0类为正类，第1类为反类，$D^+、D^-$代表正、反例子集，则**代价敏感**（cost-sensitve）错误率为：

$$\begin{aligned} E(f ; D ; \text {cost) })=& \frac{1}{m}\left(\sum_{x_{i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right) \times \cos t_{01}\right.\\ &+\sum_{\boldsymbol{x}_{i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right) \times \operatorname{cost}_{10} ) \end{aligned}$$

在非均等状态下，**“代价曲线”**（cost curve）能够反映学习器的期望总体代价（ROC不能）。

代价曲线横轴为取值[0,1]的正例率代价：

$$P(+) \cos t=\frac{p \times \cos t_{01}}{p \times \cos t_{01}+(1-p) \times \cos t_{10}}$$

其中$p$是样例为正的概率。纵轴为取值[0,1]的归一化代价（FPR是假正例率，FNR = 1-TPR是假反例率）：

$$\operatorname{cost}_{n o r m}=\frac{\operatorname{FNR} \times p \times \cos t_{01}+\operatorname{FPR} \times(1-p) \times \cos t_{10}}{p \times \operatorname{cost}_{01}+(1-p) \times \operatorname{cost}_{10}}$$

***代价曲线的绘制***：依据ROC上每个点计算出（FPR,TPR）,得到FNR，在代价平面上绘制一条从（0,FPR）到（1,FNR）的线段，线段下的面积为该条件下的期望总体代价。将ROC上每个点转化为线段，所有线段下的面积为在所有条件下学习器的期望总体代价。

![](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-16-ML2/07.png)

## 比较检验

如何比较性能度量结果？——统计假设检验（hypothesis test）。

## 偏差与方差

**偏差-方差分解**（bias-variance decomposition）是解释学习算法泛化性能的重要工具。

对测试样本$x$，令$y_D$为$x$在数据集中的标记，$y$为真实标记，$f(x;D)$为训练集D上学得模型$f$在$x$上的预测输出。回归任务为例，学习算法的期望误差为：

$$\overline{f}(\boldsymbol{x})=\mathbb{E}_{D}[f(\boldsymbol{x} ; D)]$$

样本数相同的不同训练集产生的方差为：

$$\operatorname{var}(\boldsymbol{x})=\mathbb{E}_{D}\left[(f(\boldsymbol{x} ; D)-\overline{f}(\boldsymbol{x}))^{2}\right]$$

噪声为：

$$\varepsilon^{2}=\mathbb{E}_{D}\left[\left(y_{D}-y\right)^{2}\right]$$

期望输出与真实标记的差别称为**偏差**（bias）：

$$\operatorname{bias}^{2}(\boldsymbol{x})=(\overline{f}(\boldsymbol{x})-y)^{2}$$

算法的期望误差进行分解得到：

$$E(f ; D)=\operatorname{bias}^{2}(\boldsymbol{x})+\operatorname{var}(\boldsymbol{x})+\varepsilon^{2}$$

泛化误差被分解为偏差、方差与噪声之和。

偏差-方差分解说明泛化性能是由学习算法能力、数据充分性和学习任务本身难度共同决定的

* **偏差**度量学习算法的期望预测与真实结果的偏离程度，刻画学习算法本身的拟合能力。

* **方差**度量同样发小的训练集的变动所导致的学习性能的变化，刻画数据扰动所造成的影响。

* **噪声**表达当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画学习问题本身难度。

偏差和方差是有冲突的，偏差递减，方差递增。




> 参考：周志华《机器学习》第二章



<br>