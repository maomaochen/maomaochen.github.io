---
layout:     post
title:      周志华《机器学习》第一章笔记
subtitle:   NP-Q
date:       2019-4-15
author:     maomaochen
header-img: img/post-bg-rwd.jpg
keywords_post:  "ML,周志华,笔记"
catalog: true
tags:
    - ML
    - 读书笔记
    - 《机器学习》周志华
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 第一章 绪论

## 引言

> 能做出有效的预判，是因为已经积累了许多经验，通过对经验的利用可以对新情况做出有效的决策。

机器学习是一门通过计算手段，利用经验来改善系统自身性能的学科。“经验”通常以**“数据”**形式存在，所以机器学习的主要研究内容是在计算机上从数据产生“模型”（model）的算法——**“学习算法”**（learning algorithm）。将经验数据提供给学习算法，就能基于数据产生模型，面对新的情况时模型会给我们提供相应的判断。

> 例如：有一个没剖开的西瓜，根据已有的西瓜形态特征，判断这是一个好瓜还是坏瓜。

## 基本术语

根据训练数据是否拥有标记信息，学习任务可以大致分为两类：

1. 监督学习（supervised learning）
   * 分类（classification）-离散
   * 回归（regression）-连续
2. 无监督学习（unsupervised learning）
   * 聚类（clustering）

***注意：***机器学习的目标是使学得的模型能很好的适应“新样本”，而不仅仅只是在训练样本上效果好。这种学得模型适用于新样本的能力，被称为**“泛化”**（generalization）能力

训练集要能很好的反映样本空间的特性，否则很难相信在该训练集上学得的模型能在整个样本空间上很好的工作。

> 假设样本空间中全体样本服从一个未知“分布”（distribution）*D*,我们获得的每个样本都是独立的从这个分布上采样获得的，即**“独立同分布”**（independent and identically distributed）,一般而言，训练样本越多，得到的关于*D*的信息越多，越有可能通过学习得到泛化能力强的模型。
>
> > 参考[百度百科-独立同分布](https://baike.baidu.com/link?url=Qdq-O4Oxe5TpJ4nKh25LHGZz-pod5p28pUi0NzxhFl1nKPldiK8bAuBh4k_lz0BJOMfL3PbqBjH8r3MjEeIy9xjf__vXjPoECh-ae07Qcr5ixoraXaTd4N133G3yVG1TDQnkrFinkNKpMD_jiyYmqK)
> >
> > 1. 独立：每次抽样之间没有关系，不会互相影响
> > 2. 同分布：每次抽样，样本服从同一个分布
> >    * 给一个骰子，每次抛骰子得到任意点数的概率都是1/6，这就是同分布
> > 3. 独立同分布：每次抽样之间独立且同分布

## 假设空间

1. 归纳（induction）：从特殊到一半的**泛化**过程，从事实得到一般性规律
2. 演绎（generalization）：从一半到特殊的**特化**过程，从基础原理得到具体情况

> 从样例中学习是一个归纳过程，即**“归纳学习”**（inductive learning）。

归纳学习：

* 广义：从样例中学习
* 狭义：从训练数据中学得概念——“概念学习”（目前研究应用少，现实常用的技术是产生“黑箱”模型）

概念学习最基本的是**布尔学习**。下图为一个简单例子：

![西瓜数据集](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-15-ML1/01.png)

假设好瓜由色泽、根蒂、敲声三个因素完全确定，学得的将是“好瓜是某种色泽、某种根蒂、某种敲声的瓜”这样的概念，用布尔式表达为`好瓜<->（色泽=？）∧（根蒂=？）∧（敲声=？）`，其中`“？”`是未确定的值，通过对表1.1的学习可以把`“？”`确定下来。

学习过程可以看作是一个在所有假设（hypothesis）组成的空间中进行搜索的过程，目标是找到与训练集“匹配”（fit）的假设，能够在训练集上判断正确的假设。假设一点确定，假设空间极其规模就确定了。在上述例子中，假设空间由`（色泽=？）∧（根蒂=？）∧（敲声=？）`的可能取值所形成的假设组成。考虑到每个属性的取值或者无论什么取值都合适（用`*`表示），以及极端情况"好瓜"概念不成立（用`∅`表示）。这样色泽、根蒂、敲声分别有3/3/3种可能取值，面临的假设空间规模大小为`4×4×4+1=65`。

![西瓜问题假设空间](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-15-ML1/02.png)

搜索假设空间，不断删除与正例不一致的假设、与反例一致的假设，最终会得到与训练集一致的假设，只就是学得的结果。

> 现实问题中面临很大的假设空间，但学习过程是基于有限样本训练集进行的，所以可能有多个假设与训练集一致，这个与训练集一致的“假设集合”被称为**“版本空间”**（version space）。

## 归纳偏好

在有多个与训练集一致的假设的情况下，其对应的模型在面临新样本时，会产生不同的输出。**应该采取哪一个模型？**

对于一个具体的学习算法而言，必须要产生一个模型，学习算法的**“偏好”**会对模型的选择起到关键的作用。

> 机器学习算法在学习过程中对某种类型假设的偏好，称为**“归纳偏好”**（inductive bias），简称**“偏好”**。

任何一个机器学习算法必有其归纳偏好，否则将被假设空间中在训练集上“等效”的假设迷惑（模型预测结果不唯一）。

![多假设](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-15-ML1/03.png)

如图1.3，有两条过有限样本点的曲线，如果认为相似的样本有相似的输出，则对应学习算法偏好“平滑”的曲线A而不是“崎岖”的曲线B。

一般性的引导算法确立“正确的”偏好的原则是**“奥卡姆剃刀”**（Occam's razor）——`“若有多个假设与观察一致，选最简单的那个”`。奥卡姆剃刀不是唯一可行的原则。

> 归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设。算法的归纳偏好要与问题本身匹配，假设才能成立。这在大多数时候直接决定算法的性能。

![没有免费的午餐](https://raw.githubusercontent.com/maomaochen/imguse/master/2019-04-15-ML1/04.png)

对于一个学习算法a,它在某一些问题上比学习算法b好，则存在另一些问题，算法b比a好。`此结论对任何算法成立`。

假设样本空间$$\mathcal{X}​$$和假设空间$$\mathcal{H}​$$都是离散的，令$$P\left(h | X, \mathfrak{L}_{a}\right)​$$为算法$$\mathfrak{L}_{a}​$$基于训练数据$$X​$$产生假设$$h​$$的概率，令$$f​$$代表希望学习的真实目标函数。$\mathfrak{L}_{a}​$的“训练集外误差”，即$\mathfrak{L}_{a}​$在训练集之外的所有样本上的误差为：

\$$E_{o t e}\left(\mathfrak{L}_{a} | X, f\right)=\sum_{h} \sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h | X, \mathfrak{L}_{a}\right)​$$

>$\mathbb{I}(\cdot)​$为指示函数，·为真取1，为假取0。

考虑二分类问题，真实目标函数为任意函数$\mathcal{X} \mapsto\{0,1\}​$，函数空间为$\{0,1\}|\mathcal{X}|​$。对所有可能的$f​$按均匀分布对误差求和，有：
$$
\begin{aligned} \sum_{f} E_{o t e}\left(\mathfrak{L}_{a} | X, f\right) &=\sum_{f} \sum_{h} \sum_{x \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h | X, \mathfrak{L}_{a}\right) \\ &=\sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum_{h} P\left(h | X, \mathfrak{L}_{a}\right) \sum_{f} \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) \\ &=\sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum_{h} P\left(h | X, \mathfrak{L}_{a}\right) \frac{1}{2} 2^{|\chi|} \\ &=\frac{1}{2} 2^{|\mathcal{X}|} \sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum_{h} P\left(h | X, \mathfrak{L}_{a}\right) \\ &= 2^{|\mathcal{X}|-1} \sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \cdot 1 \end{aligned},（1.2）
$$
公式1.2显示，总误差与学习算法无关，对于任意两个算法$\mathfrak{L}_{a}$和$\mathfrak{L}_{b}$都有：

$$
\sum_{f} E_{o t e}\left(\mathfrak{L}_{a} | X, f\right)=\sum_{f} E_{o t e}\left(\mathfrak{L}_{b} | X, f\right)，（1.3）
$$
无论$\mathfrak{L}_{a}$有多好、$\mathfrak{L}_{b}$有多坏，它们的期望性能是一样的，只就是**“没有免费的午餐”定理**（No Free Lunch Theorem,NFL）。

> NFL定理的重要前提是所有“问题”出现的机会相同，或所有问题同等重要。但实际情况并不是这样

**NFL的重要寓意：**脱离具体问题，空谈“什么学习算法更好”毫无意义。讨论算法的相对优劣，必须针对具体的学习问题。

E_{o t e}\left(\mathfrak{L}_{a} | X, f\right)=\sum_{h} \sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h | X, \mathfrak{L}_{a}\right)



> 参考：周志华《机器学习》第一章



<br>